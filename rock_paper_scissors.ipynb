{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiV9urRMDIE7xYCihTziG+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GitfishYu/rock-paper-scissors-MachineLearning/blob/main/rock_paper_scissors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2spberL0qwav",
        "outputId": "8be57d22-98b9-4c1b-d64e-5c601c3163dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.2\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 0s 0us/step\n",
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.4964\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3730\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3320\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3112\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2920\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3635\n",
            "313/313 [==============================] - 1s 2ms/step\n",
            "[1.2985909e-06 1.9944957e-06 7.5935887e-08 8.6922751e-07 3.4891087e-07\n",
            " 1.2655144e-03 3.0547227e-07 2.7763274e-02 2.8174485e-05 9.7093815e-01]\n",
            "9\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(training_images, training_labels) ,  (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "training_images = training_images/255.0\n",
        "test_images = test_images/255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
        "\n",
        "model.compile(optimizer = 'adam',\n",
        "              loss = 'sparse_categorical_crossentropy')\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classifications = model.predict(test_images)\n",
        "\n",
        "print(classifications[0])\n",
        "print(test_labels[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip \\\n",
        "  -O /tmp/rps.zip\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip \\\n",
        "  -O /tmp/rps-test-set.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9JPp8hbB8Rrs",
        "outputId": "ce50f80a-dfd2-42b5-925e-98577a0102bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-10-16 18:28:17--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.97.128, 108.177.125.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 200682221 (191M) [application/zip]\n",
            "Saving to: ‘/tmp/rps.zip’\n",
            "\n",
            "/tmp/rps.zip        100%[===================>] 191.38M  29.8MB/s    in 6.4s    \n",
            "\n",
            "2022-10-16 18:28:24 (29.8 MB/s) - ‘/tmp/rps.zip’ saved [200682221/200682221]\n",
            "\n",
            "--2022-10-16 18:28:24--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.97.128, 108.177.125.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29516758 (28M) [application/zip]\n",
            "Saving to: ‘/tmp/rps-test-set.zip’\n",
            "\n",
            "/tmp/rps-test-set.z 100%[===================>]  28.15M  49.0MB/s    in 0.6s    \n",
            "\n",
            "2022-10-16 18:28:26 (49.0 MB/s) - ‘/tmp/rps-test-set.zip’ saved [29516758/29516758]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "\n",
        "local_zip = '/tmp/rps.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()\n",
        "\n",
        "local_zip = '/tmp/rps-test-set.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp/')\n",
        "zip_ref.close()\n",
        "\n",
        "TRAINING_DIR = \"/tmp/rps/\"\n",
        "training_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train_generator = training_datagen.flow_from_directory(\n",
        "    TRAINING_DIR,\n",
        "    target_size=(150,150),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "TESTING_DIR = \"/tmp/rps-test-set/\"\n",
        "testing_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "validation_generator = testing_datagen.flow_from_directory(\n",
        "    TESTING_DIR,\n",
        "    target_size=(150,150),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    # Four layers of convolutions, each with MaxPooling\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    # Improve the efficiency of a neural network by throwing away some of the neurons\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    # Three classes: rock, paper, scissors\n",
        "    tf.keras.layers.Dense(3, activation='softmax')                             \n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_generator, epochs=25,\n",
        "                              validation_data=validation_generator,\n",
        "                              verbose = 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_Uct6Pw2nCl",
        "outputId": "2b1deb80-51a9-4a60-b054-49d9c6435724"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2520 images belonging to 3 classes.\n",
            "Found 372 images belonging to 3 classes.\n",
            "Epoch 1/25\n",
            "79/79 [==============================] - 185s 2s/step - loss: 0.8451 - accuracy: 0.6853 - val_loss: 0.6316 - val_accuracy: 0.8333\n",
            "Epoch 2/25\n",
            "79/79 [==============================] - 186s 2s/step - loss: 0.0492 - accuracy: 0.9829 - val_loss: 0.5631 - val_accuracy: 0.8978\n",
            "Epoch 3/25\n",
            "79/79 [==============================] - 184s 2s/step - loss: 0.1248 - accuracy: 0.9810 - val_loss: 0.7218 - val_accuracy: 0.8817\n",
            "Epoch 4/25\n",
            "79/79 [==============================] - 184s 2s/step - loss: 7.1908e-04 - accuracy: 1.0000 - val_loss: 0.8574 - val_accuracy: 0.9247\n",
            "Epoch 5/25\n",
            "79/79 [==============================] - 186s 2s/step - loss: 0.0134 - accuracy: 0.9984 - val_loss: 1.8031 - val_accuracy: 0.8414\n",
            "Epoch 6/25\n",
            "79/79 [==============================] - 186s 2s/step - loss: 0.0273 - accuracy: 0.9944 - val_loss: 0.9866 - val_accuracy: 0.8683\n",
            "Epoch 7/25\n",
            "79/79 [==============================] - 186s 2s/step - loss: 2.3072e-05 - accuracy: 1.0000 - val_loss: 1.3127 - val_accuracy: 0.9032\n",
            "Epoch 8/25\n",
            "79/79 [==============================] - 188s 2s/step - loss: 9.8581e-08 - accuracy: 1.0000 - val_loss: 1.7007 - val_accuracy: 0.8871\n",
            "Epoch 9/25\n",
            "79/79 [==============================] - 186s 2s/step - loss: 0.0164 - accuracy: 0.9964 - val_loss: 1.7919 - val_accuracy: 0.8629\n",
            "Epoch 10/25\n",
            "79/79 [==============================] - 186s 2s/step - loss: 0.0160 - accuracy: 0.9948 - val_loss: 3.6278 - val_accuracy: 0.8306\n",
            "Epoch 11/25\n",
            "79/79 [==============================] - 186s 2s/step - loss: 0.0215 - accuracy: 0.9964 - val_loss: 2.5008 - val_accuracy: 0.8118\n",
            "Epoch 12/25\n",
            "79/79 [==============================] - 186s 2s/step - loss: 3.0252e-04 - accuracy: 1.0000 - val_loss: 2.9149 - val_accuracy: 0.8495\n",
            "Epoch 13/25\n",
            "79/79 [==============================] - 185s 2s/step - loss: 5.6102e-08 - accuracy: 1.0000 - val_loss: 2.8685 - val_accuracy: 0.8629\n",
            "Epoch 14/25\n",
            "79/79 [==============================] - 186s 2s/step - loss: 0.0088 - accuracy: 0.9984 - val_loss: 4.4538 - val_accuracy: 0.8387\n",
            "Epoch 15/25\n",
            "79/79 [==============================] - 185s 2s/step - loss: 1.0644e-08 - accuracy: 1.0000 - val_loss: 4.5133 - val_accuracy: 0.8280\n",
            "Epoch 16/25\n",
            "79/79 [==============================] - 186s 2s/step - loss: 0.0029 - accuracy: 0.9996 - val_loss: 8.7691 - val_accuracy: 0.7285\n",
            "Epoch 17/25\n",
            "79/79 [==============================] - 188s 2s/step - loss: 5.1092e-04 - accuracy: 0.9996 - val_loss: 3.5205 - val_accuracy: 0.8575\n",
            "Epoch 18/25\n",
            "79/79 [==============================] - 186s 2s/step - loss: 0.0312 - accuracy: 0.9968 - val_loss: 3.3741 - val_accuracy: 0.8468\n",
            "Epoch 19/25\n",
            "79/79 [==============================] - 187s 2s/step - loss: 1.8370e-04 - accuracy: 1.0000 - val_loss: 4.3129 - val_accuracy: 0.8414\n",
            "Epoch 20/25\n",
            "79/79 [==============================] - 186s 2s/step - loss: 3.6425e-09 - accuracy: 1.0000 - val_loss: 4.3285 - val_accuracy: 0.8441\n",
            "Epoch 21/25\n",
            "79/79 [==============================] - 186s 2s/step - loss: 0.0130 - accuracy: 0.9992 - val_loss: 4.0748 - val_accuracy: 0.8387\n",
            "Epoch 22/25\n",
            "79/79 [==============================] - 185s 2s/step - loss: 2.2754e-08 - accuracy: 1.0000 - val_loss: 4.1208 - val_accuracy: 0.8360\n",
            "Epoch 23/25\n",
            "79/79 [==============================] - 186s 2s/step - loss: 1.1968e-08 - accuracy: 1.0000 - val_loss: 4.4380 - val_accuracy: 0.8333\n",
            "Epoch 24/25\n",
            "79/79 [==============================] - 185s 2s/step - loss: 2.1651e-05 - accuracy: 1.0000 - val_loss: 6.1723 - val_accuracy: 0.7876\n",
            "Epoch 25/25\n",
            "79/79 [==============================] - 185s 2s/step - loss: 0.0077 - accuracy: 0.9996 - val_loss: 5.8613 - val_accuracy: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZxWwF66qxw_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}